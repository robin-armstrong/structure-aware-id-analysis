using LinearAlgebra
using Distributions
using StatsBase
using Random
using PyPlot
using JLD2

include("../../algorithms/rgks.jl")
include("../../algorithms/rcpqr.jl")
include("../../algorithms/rsvd.jl")
include("../../algorithms/levg.jl")
include("../../utilities/plot_config.jl")

####################################################################
##################### SCRIPT PARAMETERS ############################
####################################################################

# DEFAULT PARAMETER SETTINGS:
#
# rng          = MersenneTwister(1)
# num_clusters = 1
# num_points   = 2000
# radius       = 0
# noise        = 5
# bandwidth    = 2
# normalize    = false
# krange       = 1:2:99
# k_plot       = 80
# k_draw       = 25
# numtrials    = 50

# path prefix for all output generated by this script
destination = "src/experiments/kernel_example/onecomponent"
readme      = "Testing low-rank approximation algorithms on a kernel evaluation matrix, generated from a Gaussian mixture model."

rng          = MersenneTwister(1)   # random souce, set for reproducibility
num_clusters = 1                    # number of clusters in mixture model
num_points   = 2000                 # number of points per cluster
radius       = 0                    # clusters are equispaced around a circle of this radius
noise        = 5                    # standard deviation of Gaussian mixture components
bandwidth    = 2                    # bandwidth of Gaussian kernel
normalize    = false
krange       = 1:2:99               # range of approximation ranks to test
k_plot       = 80
k_draw       = 25
numtrials    = 50                   # trials per approximation rank

plot_only = false

####################################################################
##################### DATA GENERATION ##############################
####################################################################

function fprintln(s)
    println(s)
    flush(stdout)
end

if(!plot_only)
    function run_kernel_example(destination, readme, rng, num_clusters, num_points, radius, noise, bandwidth, krange, numtrials, plot_only)
        logstr  = readme*"\n\n"
        logstr *= "rng          = "*string(rng)*"\n"
        logstr *= "num_clusters = "*string(num_clusters)*"\n"
        logstr *= "num_points   = "*string(num_points)*"\n"
        logstr *= "radius       = "*string(radius)*"\n"
        logstr *= "noise        = "*string(noise)*"\n"
        logstr *= "bandwidth    = "*string(bandwidth)*"\n"
        logstr *= "normalize    = "*string(normalize)*"\n"
        logstr *= "krange       = "*string(krange)*"\n"
        logstr *= "numtrials    = "*string(numtrials)*"\n"

        fprintln("\n"*logstr)
        
        logfile = destination*"_log.txt"
        touch(logfile)
        io = open(logfile, "w")
        write(logfile, logstr)
        close(io)

        angles        = range(0, 2*pi, num_clusters + 1)
        centers       = zeros(2, num_clusters)
        centers[1, :] = radius*cos.(angles[1:num_clusters])
        centers[2, :] = radius*sin.(angles[1:num_clusters])

        N       = num_clusters*num_points
        points  = zeros(2, N)
        sqnorms = zeros(N)
        
        fprintln("generating data points...")

        for i = 1:N
            c            = 1 + floor(Int64, num_clusters*rand(rng))
            points[:, i] = centers[:, c] .+ noise*randn(rng, 2)
            sqnorms[i]   = norm(points[:, i]).^2
        end

        fprintln("evaluating kernel matrix...")

        K  = zeros(N, N)
        K += sqnorms*ones(1, N)
        K += ones(N)*sqnorms'
        K -= 2*points'*points
        K /= -2*bandwidth^2
        broadcast!(exp, K, K)

        if(normalize)
            D = inv(Diagonal(sqrt.(K*ones(N))))
            lmul!(D, K)
            rmul!(K, D)
        end

        fprintln("computing SVD of kernel matrix...")

        kernel_svd = svd(K)

        data   = Dict()
        means  = Dict()
        stds   = Dict()
        quants = Dict()

        for alg in ["dgeqp3", "levg", "rcpqr", "rgks"]
            data[alg]   = zeros(length(krange), numtrials)
            means[alg]  = zeros(length(krange))
            stds[alg]   = zeros(length(krange))
            quants[alg] = zeros(length(krange), 2)
        end

        fprintln("running tests...\n")

        trialcounter = 0

        for t = 1:numtrials
            for i = 1:length(krange)
                trialcounter += 1
                
                k       = krange[i]
                lscores = [norm(kernel_svd.Vt[1:k, j])^2 for j = 1:size(K, 2)]
                lscores = min.(lscores, 1.)
                lscores = max.(lscores, 0.)

                if(trialcounter % 10 == 0)
                    fprintln("   trial "*string(trialcounter)*" out of "*string(numtrials*length(krange)))
                end

                r1 = rgks(rng, K, k, oversamp = ceil(Int64, .1*k))
                r2 = rcpqr(rng, K, k, oversamp = ceil(Int64, .1*k))
                r3 = levg(rng, K, k, oversamp = ceil(Int64, .1*k), leverage_scores = lscores)
                r4 = qr(K, ColumnNorm())

                # optimally reducing the levg approximation to rank k

                U = svd(r3.X).U
                Q = r3.Q*U[:, 1:k]
                X = Q'*K

                # finding the approximation subspace used by DGEQP3
                W = r4.Q*Matrix{Float64}(I(N)[:, 1:k])

                data["rgks"][i, t]    = norm(K - (r1.Q)*(r1.X))
                data["rcpqr"][i, t]     = norm(K - (r2.Q)*(r2.X))
                data["levg"][i, t]    = norm(K - Q*X)
                data["dgeqp3"][i, t]  = norm(K - W*(W'*K))
            end

            @save destination*"_data.jld2" krange numtrials points kernel_svd data means stds quants
        end

        fprintln("\ncalculating approximation error statistics...")
        
        for alg in ["dgeqp3", "levg", "rcpqr", "rgks"]
            means[alg]  = vec(mean(data[alg], dims = 2))
            stds[alg]   = vec(std(data[alg], dims = 2))

            for i = 1:length(krange)
                quants[alg][i, 1] = quantile(data[alg][i, :], .05)
                quants[alg][i, 2] = quantile(data[alg][i, :], .95)
            end
        end

        @save destination*"_data.jld2" krange numtrials points kernel_svd data means stds quants
    end

    run_kernel_example(destination, readme, rng, num_clusters, num_points, radius, noise, bandwidth, krange, numtrials, plot_only)
end

####################################################################
##################### PLOTTING #####################################
####################################################################

@load destination*"_data.jld2" krange numtrials points kernel_svd data means stds quants

fprintln("plotting error statistics...")

kernel_norm = norm(kernel_svd.S)
optimal     = [norm(kernel_svd.S[(k + 1):end]) for k in krange]

ioff()
fig, (norm_rel, opt_rel) = subplots(1, 2, figsize = (10, 4))

mfreq      = 5
errbar     = "confidence"   # either "confidence" or "quantile"
confidence = .95

alpha = quantile(Normal(0, 1), 1 - .5*(1 - confidence))

norm_rel.set_xlabel(L"Approximation Rank ($k$)")
norm_rel.set_ylabel("Relative Frobenius Error")
norm_rel.set_yscale("log")

opt_rel.set_xlabel(L"Approximation Rank ($k$)")
opt_rel.set_ylabel("Frobenius Error Suboptimality")
opt_rel.set_ylim([1., 5.])

for alg in ["dgeqp3", "levg", "rcpqr", "rgks"]
    norm_rel.plot(krange, means[alg]/kernel_norm, color = algcolors[alg], marker = algmarkers[alg], markevery = mfreq, markerfacecolor = "none", label = alglabels[alg])
    opt_rel.plot(krange, means[alg]./optimal, color = algcolors[alg], marker = algmarkers[alg], markevery = mfreq, markerfacecolor = "none")

    if(errbar == "quantile")
        norm_rel.fill_between(krange, quants[alg][:, 1]/kernel_norm, quants[alg][:, 2]/kernel_norm, color = algcolors[alg], alpha = .2)
        opt_rel.fill_between(krange, quants[alg][:, 1]./optimal, quants[alg][:, 2]./optimal, color = algcolors[alg], alpha = .2)
    elseif(errbar == "confidence")
        norm_rel.fill_between(krange, (means[alg] .+ alpha*stds[alg]/sqrt(numtrials))/kernel_norm, (means[alg] .- alpha*stds[alg]/sqrt(numtrials))/kernel_norm, color = algcolors[alg], alpha = .2)
        opt_rel.fill_between(krange, (means[alg] .+ alpha*stds[alg]/sqrt(numtrials))./optimal, (means[alg] .- alpha*stds[alg]/sqrt(numtrials))./optimal, color = algcolors[alg], alpha = .2)
    else
        throw(ArgumentError("unrecognized error bar type, '"*errbar*"'"))
    end
end

norm_rel.plot(krange, optimal/kernel_norm, color = "black", linestyle = "dashed", label = "Optimal")
norm_rel.legend()

savefig(destination*"_error_plot.pdf", bbox_inches = "tight")
close(fig)

K       = kernel_svd.U*Diagonal(kernel_svd.S)*kernel_svd.Vt
p_cpqr  = rcpqr(rng, K, k_plot, oversamp = ceil(Int64, .1*k_plot)).p[1:k_draw]
p_gks   = rgks(rng, K, k_plot, oversamp = ceil(Int64, .1*k_plot)).p[1:k_draw]

colnorms = [norm(K[:, j]) for j = 1:size(K, 2)]
l_scores = [norm(kernel_svd.Vt[1:k_plot, j]) for j = 1:size(K, 2)]

fig, (norm_levg, cloud) = subplots(1, 2, figsize = (8, 4))

norm_levg.set_xlabel(L"Leverage Score ($k = 80$)")
norm_levg.set_ylabel("Column Norm")

norm_levg.scatter(l_scores, colnorms, color = "black", alpha = .1)
cloud.scatter(points[1, :], points[2, :], color = "black", alpha = .1)

norm_levg.scatter(l_scores[p_cpqr], colnorms[p_cpqr], color = "limegreen", marker = "s", label = "RCPQR")
norm_levg.scatter(l_scores[p_gks], colnorms[p_gks], color = "fuchsia", marker = "D", label = "RGKS")
cloud.scatter(points[1, p_cpqr], points[2, p_cpqr], color = "limegreen", marker = "s", label = "RCPQR")
cloud.scatter(points[1, p_gks], points[2, p_gks], color = "fuchsia", marker = "D", label = "RCPQR")
norm_levg.legend()

savefig(destination*"_scatter_plot.pdf", bbox_inches = "tight")
close(fig)
