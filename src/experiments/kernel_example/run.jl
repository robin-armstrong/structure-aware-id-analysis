using LinearAlgebra
using Distributions
using StatsBase
using Random
using PyPlot
using JLD2

include("../../algorithms/rgks.jl")
include("../../algorithms/rid.jl")
include("../../algorithms/rsvd.jl")
include("../../algorithms/levg.jl")
include("../../utilities/plot_config.jl")

####################################################################
##################### SCRIPT PARAMETERS ############################
####################################################################

# path prefix for all output generated by this script
destination = "src/experiments/kernel_example/noisymixture"
readme      = "An experiment testing two ID algorithms on a kernel evaluation matrix, generated from a noisy Gaussian mixture model."

rng          = MersenneTwister(1)   # random souce, set for reproducibility
num_clusters = 10                   # number of clusters in mixture model
num_points   = 200                  # number of points per cluster
radius       = 5                    # clusters are equispaced around a circle of this radius
noise        = 3                    # standard deviation of Gaussian mixture components
bandwidth    = 2                    # bandwidth of Gaussian kernel
krange       = 1:2:100              # range of approximation ranks to test
numtrials    = 300                  # trials per approximation rank

plot_only = false

####################################################################
##################### DATA GENERATION ##############################
####################################################################

if(!plot_only)
    function run_kernel_example(destination, readme, rng, num_clusters, num_points, radius, noise, bandwidth, krange, numtrials, plot_only)
        logstr  = readme*"\n\n"
        logstr *= "rng          = "*string(rng)*"\n"
        logstr *= "num_clusters = "*string(num_clusters)*"\n"
        logstr *= "num_points   = "*string(num_points)*"\n"
        logstr *= "radius       = "*string(radius)*"\n"
        logstr *= "noise        = "*string(noise)*"\n"
        logstr *= "bandwidth    = "*string(bandwidth)*"\n"
        logstr *= "krange       = "*string(krange)*"\n"
        logstr *= "numtrials    = "*string(numtrials)*"\n"

        println("\n"*logstr)
        
        logfile = destination*"_log.txt"
        touch(logfile)
        io = open(logfile, "w")
        write(logfile, logstr)
        close(io)

        angles        = range(0, 2*pi, num_clusters + 1)
        centers       = zeros(2, num_clusters)
        centers[1, :] = radius*cos.(angles[1:num_clusters])
        centers[2, :] = radius*sin.(angles[1:num_clusters])

        N       = num_clusters*num_points
        points  = zeros(2, N)
        sqnorms = zeros(N)
        
        println("generating data points...")

        for i = 1:N
            c            = 1 + floor(Int64, num_clusters*rand(rng))
            points[:, i] = centers[:, c] .+ noise*randn(rng, 2)
            sqnorms[i]   = norm(points[:, i]).^2
        end

        println("evaluating kernel matrix...")

        K  = zeros(N, N)
        K += sqnorms*ones(1, N)
        K += ones(N)*sqnorms'
        K -= 2*points'*points
        K /= -2*bandwidth^2
        broadcast!(exp, K, K)

        println("computing SVD of kernel matrix...")

        kernel_svd = svd(K)

        data = Dict()

        for alg in algnames
            data[alg] = zeros(length(krange), numtrials)
        end

        println("running tests...\n")

        trialcounter = 0

        for i = 1:length(krange)
            k       = krange[i]
            lscores = [norm(kernel_svd.Vt[1:k, j])^2 for j = 1:size(K, 2)]

            for t = 1:numtrials
                trialcounter += 1

                if(trialcounter % 10 == 0)
                    println("   trial "*string(trialcounter)*" out of "*string(numtrials*length(krange)))
                end

                r1 = rgks(rng, K, k, oversamp = ceil(Int64, .1*k))
                r2 = rid(rng, K, k, oversamp = ceil(Int64, .1*k))
                r3 = rsvd(rng, K, k, oversamp = ceil(Int64, .1*k), power = 0)
                r4 = rsvd(rng, K, k, oversamp = ceil(Int64, .1*k), power = 1)
                r5 = levg(rng, K, k, oversamp = ceil(Int64, .1*k), leverage_scores = lscores)

                data["rgks"][i, t]    = norm(K - (r1.Q)*(r1.X))
                data["rid"][i, t]     = norm(K - (r2.Q)*(r2.X))
                data["rsvd_q0"][i, t] = norm(K - (r3.U)*Diagonal(r3.S)*(r3.Vt))
                data["rsvd_q1"][i, t] = norm(K - (r4.U)*Diagonal(r4.S)*(r4.Vt))
                data["levg"][i, t]    = norm(K - (r5.Q)*(r5.X))
            end
        end

        println("\ncalculating approximation error statistics...")
        
        means  = Dict()
        stds   = Dict()
        quants = Dict()

        for alg in algnames
            means[alg]  = vec(mean(data[alg], dims = 2))
            stds[alg]   = vec(std(data[alg], dims = 2))
            quants[alg] = zeros(length(krange), 2)

            for i = 1:length(krange)
                quants[alg][i, 1] = quantile(data[alg][i, :], .05)
                quants[alg][i, 2] = quantile(data[alg][i, :], .95)
            end
        end

        @save destination*"_data.jld2" krange numtrials kernel_svd data means stds quants
    end

    run_kernel_example(destination, readme, rng, num_clusters, num_points, radius, noise, bandwidth, krange, numtrials, plot_only)
end

####################################################################
##################### PLOTTING #####################################
####################################################################

@load destination*"_data.jld2" krange numtrials kernel_svd data means stds quants

kernel_norm = norm(kernel_svd.S)
optimal     = [norm(kernel_svd.S[(k + 1):end]) for k in krange]

ioff()
fig, (norm_rel, opt_rel) = subplots(1, 2, figsize = (10, 4))

mfreq      = 5
errbar     = "confidence"   # either "confidence" or "quantile"
confidence = .95

alpha = quantile(Normal(0, 1), 1 - .5*(1 - confidence))

norm_rel.set_xlabel(L"Approximation Rank ($k$)")
norm_rel.set_ylabel("Relative Frobenius Error")
norm_rel.set_yscale("log")

opt_rel.set_xlabel(L"Approximation Rank ($k$)")
opt_rel.set_ylabel("Frobenius Error Suboptimality")
opt_rel.set_ylim([1., 3.])

for alg in algnames
    norm_rel.plot(krange, means[alg]/kernel_norm, color = algcolors[alg], marker = algmarkers[alg], markevery = mfreq, markerfacecolor = "none", label = alglabels[alg])
    opt_rel.plot(krange, means[alg]./optimal, color = algcolors[alg], marker = algmarkers[alg], markevery = mfreq, markerfacecolor = "none")

    if(errbar == "quantile")
        norm_rel.fill_between(krange, quants[alg][:, 1]/kernel_norm, quants[alg][:, 2]/kernel_norm, color = algcolors[alg], alpha = .2)
        opt_rel.fill_between(krange, quants[alg][:, 1]./optimal, quants[alg][:, 2]./optimal, color = algcolors[alg], alpha = .2)
    elseif(errbar == "confidence")
        norm_rel.fill_between(krange, (means[alg] .+ alpha*stds[alg]/sqrt(numtrials))/kernel_norm, (means[alg] .- alpha*stds[alg]/sqrt(numtrials))/kernel_norm, color = algcolors[alg], alpha = .2)
        opt_rel.fill_between(krange, (means[alg] .+ alpha*stds[alg]/sqrt(numtrials))./optimal, (means[alg] .- alpha*stds[alg]/sqrt(numtrials))./optimal, color = algcolors[alg], alpha = .2)
    else
        throw(ArgumentError("unrecognized error bar type, '"*errbar*"'"))
    end
end

norm_rel.plot(krange, optimal/kernel_norm, color = "black", linestyle = "dashed", label = "Optimal")
norm_rel.legend()

savefig(destination*"_plot.pdf", bbox_inches = "tight")
close(fig)
